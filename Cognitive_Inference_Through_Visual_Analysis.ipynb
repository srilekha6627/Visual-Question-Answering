{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 3798293,
          "sourceType": "datasetVersion",
          "datasetId": 2264789
        }
      ],
      "dockerImageVersionId": 30197,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Cognitive Inference Through Visual Analysis",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'visual-question-answering-computer-vision-nlp:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2264789%2F3798293%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240522%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240522T020232Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D298f4721143ba683f807f4e0c330bba8c35418d1e4f21bdffd5faef9e3835d4126645d4c03d5a5580d729b19f92a9ecc69d691ebcc8bd2e2d0f1fd5486045c6f5110ffd4d3696f6cdceac88eca259b6888cc05f83ef2bdbce9ea3db8904cc82b1902bcd5652e645d8b8358ad1196fb2bbdd8acbc888bf600eb8fa2c19b6f444827d436bbce4067451f1e16f746a00e392226c083d722dcaaae56be7262415626b6cb3b44b65626fd8f6b1c85475cdc6a9d0d1934991ac8cba704493810a0c5793a4a9c64a60c616addf8416bbb00836c5a15ef28a6ef3c30c53b7f8987133be542bcba648686fafbf624942437cdd805e08bf07bcfbf6ff49ef8d19e543e0e89'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "O2s1bFnjCTwU"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual Question Answering using Multimodal Transformer Models"
      ],
      "metadata": {
        "id": "qIuumF3_CTwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary libraries & set up the environment"
      ],
      "metadata": {
        "id": "2XLg2g91CTwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from copy import deepcopy\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from datasets import load_dataset, set_caching_enabled\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoFeatureExtractor,AutoModel, TrainingArguments, Trainer, logging\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:51:54.703979Z",
          "iopub.execute_input": "2024-04-02T16:51:54.705009Z",
          "iopub.status.idle": "2024-04-02T16:52:02.531554Z",
          "shell.execute_reply.started": "2024-04-02T16:51:54.704901Z",
          "shell.execute_reply": "2024-04-02T16:52:02.530817Z"
        },
        "trusted": true,
        "id": "EAa0M62PCTwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['HF_HOME'] = os.path.join(\".\", \"cache\")\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "set_caching_enabled(True)\n",
        "logging.set_verbosity_error()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-04-02T16:52:02.53291Z",
          "iopub.execute_input": "2024-04-02T16:52:02.533502Z",
          "iopub.status.idle": "2024-04-02T16:52:02.539096Z",
          "shell.execute_reply.started": "2024-04-02T16:52:02.533471Z",
          "shell.execute_reply": "2024-04-02T16:52:02.538174Z"
        },
        "trusted": true,
        "id": "Vkd9pcqlCTwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:52:04.189611Z",
          "iopub.execute_input": "2024-04-02T16:52:04.190008Z",
          "iopub.status.idle": "2024-04-02T16:52:04.250242Z",
          "shell.execute_reply.started": "2024-04-02T16:52:04.189976Z",
          "shell.execute_reply": "2024-04-02T16:52:04.249354Z"
        },
        "trusted": true,
        "id": "qvT0R4T6CTwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files={\n",
        "        \"train\": os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\",\"data_train.csv\"),\n",
        "        \"test\": os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\", \"data_eval.csv\")\n",
        "    }\n",
        ")\n",
        "\n",
        "with open(os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\", \"answer_space.txt\")) as f:\n",
        "    answer_space = f.read().splitlines()\n",
        "\n",
        "dataset = dataset.map(\n",
        "    lambda examples: {\n",
        "        'label': [\n",
        "            answer_space.index(ans.replace(\" \", \"\").split(\",\")[0]) # Select the 1st answer if multiple answers are provided\n",
        "            for ans in examples['answer']\n",
        "        ]\n",
        "    },\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:52:05.652379Z",
          "iopub.execute_input": "2024-04-02T16:52:05.653227Z",
          "iopub.status.idle": "2024-04-02T16:52:06.348774Z",
          "shell.execute_reply.started": "2024-04-02T16:52:05.653178Z",
          "shell.execute_reply": "2024-04-02T16:52:06.347984Z"
        },
        "trusted": true,
        "id": "qWHHHpQnCTwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('ans_space.pkl', 'wb') as file:\n",
        "    pickle.dump(answer_space, file)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:52:12.09696Z",
          "iopub.execute_input": "2024-04-02T16:52:12.09801Z",
          "iopub.status.idle": "2024-04-02T16:52:12.10381Z",
          "shell.execute_reply.started": "2024-04-02T16:52:12.097956Z",
          "shell.execute_reply": "2024-04-02T16:52:12.102705Z"
        },
        "trusted": true,
        "id": "h8xFvir1CTwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:52:17.436843Z",
          "iopub.execute_input": "2024-04-02T16:52:17.437687Z",
          "iopub.status.idle": "2024-04-02T16:52:17.441975Z",
          "shell.execute_reply.started": "2024-04-02T16:52:17.437648Z",
          "shell.execute_reply": "2024-04-02T16:52:17.441116Z"
        },
        "trusted": true,
        "id": "5TohNtTYCTwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Look at some of the Question/Image/Answer combinations"
      ],
      "metadata": {
        "id": "THQ6OM4MCTwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "def showExample(train=True, id=None):\n",
        "    if train:\n",
        "        data = dataset[\"train\"]\n",
        "    else:\n",
        "        data = dataset[\"test\"]\n",
        "    if id == None:\n",
        "        id = np.random.randint(len(data))\n",
        "    image = Image.open(os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\", \"images\", data[id][\"image_id\"] + \".png\"))\n",
        "    display(image)\n",
        "\n",
        "    print(\"Question:\\t\", data[id][\"question\"])\n",
        "    print(\"Answer:\\t\\t\", data[id][\"answer\"], \"(Label: {0})\".format(data[id][\"label\"]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:52:21.60631Z",
          "iopub.execute_input": "2024-04-02T16:52:21.607056Z",
          "iopub.status.idle": "2024-04-02T16:52:21.614801Z",
          "shell.execute_reply.started": "2024-04-02T16:52:21.607018Z",
          "shell.execute_reply": "2024-04-02T16:52:21.613876Z"
        },
        "trusted": true,
        "id": "MTTjZIa9CTwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "showExample()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:52:25.184317Z",
          "iopub.execute_input": "2024-04-02T16:52:25.184707Z",
          "iopub.status.idle": "2024-04-02T16:52:25.359597Z",
          "shell.execute_reply.started": "2024-04-02T16:52:25.184674Z",
          "shell.execute_reply": "2024-04-02T16:52:25.358837Z"
        },
        "trusted": true,
        "id": "4pj9ERLvCTwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Multimodal Collator for the Dataset"
      ],
      "metadata": {
        "id": "0IXBytVjCTwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "text = \"What is on the left side of cabinet\"\n",
        "tokens = tokenizer(text)\n",
        "input_ids = tokens[\"input_ids\"]\n",
        "attention_mask = tokens[\"attention_mask\"]\n",
        "\n",
        "print(\"Input IDs:\", input_ids)\n",
        "print(\"Attention Mask:\", attention_mask)\n",
        "print(\"token_type_ids:\", tokens['token_type_ids'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:52:26.160681Z",
          "iopub.execute_input": "2024-04-02T16:52:26.161369Z",
          "iopub.status.idle": "2024-04-02T16:52:28.499616Z",
          "shell.execute_reply.started": "2024-04-02T16:52:26.161337Z",
          "shell.execute_reply": "2024-04-02T16:52:28.498737Z"
        },
        "trusted": true,
        "id": "cL01FeQYCTwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class MultimodalCollator:\n",
        "    tokenizer: AutoTokenizer\n",
        "    preprocessor: AutoFeatureExtractor\n",
        "\n",
        "    def tokenize_text(self, texts: List[str]):\n",
        "        encoded_text = self.tokenizer(\n",
        "            text=texts,\n",
        "            padding='longest',\n",
        "            max_length=24,\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoded_text['input_ids'].squeeze(),\n",
        "            \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n",
        "            \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n",
        "        }\n",
        "\n",
        "    def preprocess_images(self, images: List[str]):\n",
        "        processed_images = self.preprocessor(\n",
        "            images=[Image.open(os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\", \"images\", image_id + \".png\")).convert('RGB') for image_id in images],\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n",
        "        }\n",
        "\n",
        "    def __call__(self, raw_batch_dict):\n",
        "        return {\n",
        "            **self.tokenize_text(\n",
        "                raw_batch_dict['question']\n",
        "                if isinstance(raw_batch_dict, dict) else\n",
        "                [i['question'] for i in raw_batch_dict]\n",
        "            ),\n",
        "            **self.preprocess_images(\n",
        "                raw_batch_dict['image_id']\n",
        "                if isinstance(raw_batch_dict, dict) else\n",
        "                [i['image_id'] for i in raw_batch_dict]\n",
        "            ),\n",
        "            'labels': torch.tensor(\n",
        "                raw_batch_dict['label']\n",
        "                if isinstance(raw_batch_dict, dict) else\n",
        "                [i['label'] for i in raw_batch_dict],\n",
        "                dtype=torch.int64\n",
        "            ),\n",
        "        }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:52:28.501062Z",
          "iopub.execute_input": "2024-04-02T16:52:28.501393Z",
          "iopub.status.idle": "2024-04-02T16:52:28.514065Z",
          "shell.execute_reply.started": "2024-04-02T16:52:28.501363Z",
          "shell.execute_reply": "2024-04-02T16:52:28.513232Z"
        },
        "trusted": true,
        "id": "MJmx63GUCTwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalVQAModel(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_labels: int = len(answer_space),\n",
        "            intermediate_dim: int = 512,\n",
        "            pretrained_text_name: str = 'bert-base-uncased',\n",
        "            pretrained_image_name: str = 'google/vit-base-patch16-224-in21k'):\n",
        "\n",
        "        super(MultimodalVQAModel, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.pretrained_text_name = pretrained_text_name\n",
        "        self.pretrained_image_name = pretrained_image_name\n",
        "\n",
        "        self.text_encoder = AutoModel.from_pretrained(\n",
        "            self.pretrained_text_name,\n",
        "        )\n",
        "        self.image_encoder = AutoModel.from_pretrained(\n",
        "            self.pretrained_image_name,\n",
        "        )\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size, intermediate_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids: torch.LongTensor,\n",
        "            pixel_values: torch.FloatTensor,\n",
        "            attention_mask: Optional[torch.LongTensor] = None,\n",
        "            token_type_ids: Optional[torch.LongTensor] = None,\n",
        "            labels: Optional[torch.LongTensor] = None):\n",
        "\n",
        "        encoded_text = self.text_encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        encoded_image = self.image_encoder(\n",
        "            pixel_values=pixel_values,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        fused_output = self.fusion(\n",
        "            torch.cat([encoded_text['pooler_output'], encoded_image['pooler_output'],],dim=1)\n",
        "        )\n",
        "        logits = self.classifier(fused_output)\n",
        "\n",
        "        out = {\n",
        "            \"logits\": logits\n",
        "        }\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(logits, labels)\n",
        "            out[\"loss\"] = loss\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:53:33.463267Z",
          "iopub.execute_input": "2024-04-02T16:53:33.46423Z",
          "iopub.status.idle": "2024-04-02T16:53:33.478437Z",
          "shell.execute_reply.started": "2024-04-02T16:53:33.464167Z",
          "shell.execute_reply": "2024-04-02T16:53:33.477567Z"
        },
        "trusted": true,
        "id": "D9n6bl81CTwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createMultimodalVQACollatorAndModel(text='bert-base-uncased', image='google/vit-base-patch16-224-in21k'):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(text)\n",
        "    preprocessor = AutoFeatureExtractor.from_pretrained(image)\n",
        "\n",
        "    multi_collator = MultimodalCollator(\n",
        "        tokenizer=tokenizer,\n",
        "        preprocessor=preprocessor,\n",
        "    )\n",
        "\n",
        "\n",
        "    multi_model = MultimodalVQAModel(pretrained_text_name=text, pretrained_image_name=image).to(device)\n",
        "    return multi_collator, multi_model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:53:37.563827Z",
          "iopub.execute_input": "2024-04-02T16:53:37.564542Z",
          "iopub.status.idle": "2024-04-02T16:53:37.569932Z",
          "shell.execute_reply.started": "2024-04-02T16:53:37.564504Z",
          "shell.execute_reply": "2024-04-02T16:53:37.569052Z"
        },
        "trusted": true,
        "id": "REkam4kjCTwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Metrics from Visual Question Answering"
      ],
      "metadata": {
        "id": "oNj52QFBCTwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wup_measure(a,b,similarity_threshold=0.925):\n",
        "    \"\"\"\n",
        "    Returns Wu-Palmer similarity score.\n",
        "    More specifically, it computes:\n",
        "        max_{x \\in interp(a)} max_{y \\in interp(b)} wup(x,y)\n",
        "        where interp is a 'interpretation field'\n",
        "    \"\"\"\n",
        "    def get_semantic_field(a):\n",
        "        weight = 1.0\n",
        "        semantic_field = wordnet.synsets(a,pos=wordnet.NOUN)\n",
        "        return (semantic_field,weight)\n",
        "\n",
        "\n",
        "    def get_stem_word(a):\n",
        "        \"\"\"\n",
        "        Sometimes answer has form word\\d+:wordid.\n",
        "        If so we return word and downweight\n",
        "        \"\"\"\n",
        "        weight = 1.0\n",
        "        return (a,weight)\n",
        "\n",
        "\n",
        "    global_weight=1.0\n",
        "\n",
        "    (a,global_weight_a)=get_stem_word(a)\n",
        "    (b,global_weight_b)=get_stem_word(b)\n",
        "    global_weight = min(global_weight_a,global_weight_b)\n",
        "\n",
        "    if a==b:\n",
        "        # they are the same\n",
        "        return 1.0*global_weight\n",
        "\n",
        "    if a==[] or b==[]:\n",
        "        return 0\n",
        "\n",
        "\n",
        "    interp_a,weight_a = get_semantic_field(a)\n",
        "    interp_b,weight_b = get_semantic_field(b)\n",
        "\n",
        "    if interp_a == [] or interp_b == []:\n",
        "        return 0\n",
        "\n",
        "    # we take the most optimistic interpretation\n",
        "    global_max=0.0\n",
        "    for x in interp_a:\n",
        "        for y in interp_b:\n",
        "            local_score=x.wup_similarity(y)\n",
        "            if local_score > global_max:\n",
        "                global_max=local_score\n",
        "\n",
        "    # we need to use the semantic fields and therefore we downweight\n",
        "    # unless the score is high which indicates both are synonyms\n",
        "    if global_max < similarity_threshold:\n",
        "        interp_weight = 0.1\n",
        "    else:\n",
        "        interp_weight = 1.0\n",
        "\n",
        "    final_score=global_max*weight_a*weight_b*interp_weight*global_weight\n",
        "    return final_score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:53:39.210924Z",
          "iopub.execute_input": "2024-04-02T16:53:39.211937Z",
          "iopub.status.idle": "2024-04-02T16:53:39.222553Z",
          "shell.execute_reply.started": "2024-04-02T16:53:39.211899Z",
          "shell.execute_reply": "2024-04-02T16:53:39.221725Z"
        },
        "trusted": true,
        "id": "lBe1wwCMCTwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wup_measure(a,b,similarity_threshold=0.0):\n",
        "    \"\"\"\n",
        "    Returns Wu-Palmer similarity score.\n",
        "    More specifically, it computes:\n",
        "        max_{x \\in interp(a)} max_{y \\in interp(b)} wup(x,y)\n",
        "        where interp is a 'interpretation field'\n",
        "    \"\"\"\n",
        "    def get_semantic_field(a):\n",
        "        weight = 1.0\n",
        "        semantic_field = wordnet.synsets(a,pos=wordnet.NOUN)\n",
        "        return (semantic_field,weight)\n",
        "\n",
        "\n",
        "    def get_stem_word(a):\n",
        "        \"\"\"\n",
        "        Sometimes answer has form word\\d+:wordid.\n",
        "        If so we return word and downweight\n",
        "        \"\"\"\n",
        "        weight = 1.0\n",
        "        return (a,weight)\n",
        "\n",
        "\n",
        "    global_weight=1.0\n",
        "\n",
        "    (a,global_weight_a)=get_stem_word(a)\n",
        "    (b,global_weight_b)=get_stem_word(b)\n",
        "    global_weight = min(global_weight_a,global_weight_b)\n",
        "\n",
        "    if a==b:\n",
        "        # they are the same\n",
        "        return 1.0*global_weight\n",
        "\n",
        "    if a==[] or b==[]:\n",
        "        return 0\n",
        "\n",
        "\n",
        "    interp_a,weight_a = get_semantic_field(a)\n",
        "    interp_b,weight_b = get_semantic_field(b)\n",
        "\n",
        "    if interp_a == [] or interp_b == []:\n",
        "        return 0\n",
        "\n",
        "    # we take the most optimistic interpretation\n",
        "    global_max=0.0\n",
        "    for x in interp_a:\n",
        "        for y in interp_b:\n",
        "            local_score=x.wup_similarity(y)\n",
        "            if local_score > global_max:\n",
        "                global_max=local_score\n",
        "\n",
        "    # we need to use the semantic fields and therefore we downweight\n",
        "    # unless the score is high which indicates both are synonyms\n",
        "    if global_max < similarity_threshold:\n",
        "        interp_weight = 0.1\n",
        "    else:\n",
        "        interp_weight = 1.0\n",
        "\n",
        "    final_score=global_max*weight_a*weight_b*interp_weight*global_weight\n",
        "    return final_score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:53:40.498505Z",
          "iopub.execute_input": "2024-04-02T16:53:40.498865Z",
          "iopub.status.idle": "2024-04-02T16:53:40.509868Z",
          "shell.execute_reply.started": "2024-04-02T16:53:40.498837Z",
          "shell.execute_reply": "2024-04-02T16:53:40.509077Z"
        },
        "trusted": true,
        "id": "72Nvw7TSCTwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_wup_measure(labels, preds, similarity_threshold=0.0):\n",
        "    wup_scores = [wup_measure(answer_space[label], answer_space[pred], similarity_threshold=0.0) for label, pred in zip(labels, preds)]\n",
        "    return np.mean(wup_scores)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:53:41.494547Z",
          "iopub.execute_input": "2024-04-02T16:53:41.494924Z",
          "iopub.status.idle": "2024-04-02T16:53:41.501253Z",
          "shell.execute_reply.started": "2024-04-02T16:53:41.494896Z",
          "shell.execute_reply": "2024-04-02T16:53:41.499995Z"
        },
        "trusted": true,
        "id": "Z4_TIfMhCTwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.random.randint(len(answer_space), size=5)\n",
        "preds = np.random.randint(len(answer_space), size=5)\n",
        "\n",
        "def showAnswers(ids):\n",
        "    print([answer_space[id] for id in ids])\n",
        "\n",
        "showAnswers(labels)\n",
        "showAnswers(preds)\n",
        "\n",
        "print(\"Predictions vs Labels: \", batch_wup_measure(labels, preds, similarity_threshold=0.0))\n",
        "print(\"Labels vs Labels: \", batch_wup_measure(labels, labels, similarity_threshold=0.0))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:53:42.263361Z",
          "iopub.execute_input": "2024-04-02T16:53:42.264068Z",
          "iopub.status.idle": "2024-04-02T16:53:44.634338Z",
          "shell.execute_reply.started": "2024-04-02T16:53:42.264032Z",
          "shell.execute_reply": "2024-04-02T16:53:44.633423Z"
        },
        "trusted": true,
        "id": "S1e4V8IrCTwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
        "    logits, labels = eval_tuple\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    return {\n",
        "        \"wups\": batch_wup_measure(labels, preds),\n",
        "        \"acc\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average='macro')\n",
        "    }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:53:44.635705Z",
          "iopub.execute_input": "2024-04-02T16:53:44.636014Z",
          "iopub.status.idle": "2024-04-02T16:53:44.641682Z",
          "shell.execute_reply.started": "2024-04-02T16:53:44.635986Z",
          "shell.execute_reply": "2024-04-02T16:53:44.640823Z"
        },
        "trusted": true,
        "id": "Q8q8-mrRCTwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training & Evaluation"
      ],
      "metadata": {
        "id": "oetDlPHwCTwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Arguments needed for Training"
      ],
      "metadata": {
        "id": "K4PwgASHCTwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"checkpoint\",\n",
        "    seed=12345,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,             # Save only the last 3 checkpoints at any given time while training\n",
        "    metric_for_best_model='wups',\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    remove_unused_columns=False,\n",
        "    num_train_epochs=10,\n",
        "    fp16=True,\n",
        "#     warmup_ratio=0.01,\n",
        "#     learning_rate=5e-4,\n",
        "#     weight_decay=1e-4,\n",
        "#     gradient_accumulation_steps=2,\n",
        "    dataloader_num_workers=8,\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:53:51.59775Z",
          "iopub.execute_input": "2024-04-02T16:53:51.598593Z",
          "iopub.status.idle": "2024-04-02T16:53:51.607034Z",
          "shell.execute_reply.started": "2024-04-02T16:53:51.598553Z",
          "shell.execute_reply": "2024-04-02T16:53:51.606329Z"
        },
        "trusted": true,
        "id": "qpyxq19HCTwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the Multimodal Models using User-Defined Text/Image  Transformers & Train it on the Dataset"
      ],
      "metadata": {
        "id": "I7wMheWMCTwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createAndTrainModel(dataset, args, text_model='bert-base-uncased', image_model='google/vit-base-patch16-224-in21k', multimodal_model='bert_vit'):\n",
        "    collator, model = createMultimodalVQACollatorAndModel(text_model, image_model)\n",
        "\n",
        "    multi_args = deepcopy(args)\n",
        "    multi_args.output_dir = os.path.join(\"..\", \"checkpoint\", multimodal_model)\n",
        "    multi_trainer = Trainer(\n",
        "        model,\n",
        "        multi_args,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['test'],\n",
        "        data_collator=collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    train_multi_metrics = multi_trainer.train()\n",
        "    eval_multi_metrics = multi_trainer.evaluate()\n",
        "\n",
        "    return collator, model, train_multi_metrics, eval_multi_metrics"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T16:53:54.289556Z",
          "iopub.execute_input": "2024-04-02T16:53:54.289976Z",
          "iopub.status.idle": "2024-04-02T16:53:54.29721Z",
          "shell.execute_reply.started": "2024-04-02T16:53:54.289942Z",
          "shell.execute_reply": "2024-04-02T16:53:54.29626Z"
        },
        "trusted": true,
        "id": "U9rF2loRCTwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collator, model, train_multi_metrics, eval_multi_metrics = createAndTrainModel(dataset, args)"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-04-02T16:53:56.386866Z",
          "iopub.execute_input": "2024-04-02T16:53:56.387279Z",
          "iopub.status.idle": "2024-04-02T17:48:44.077416Z",
          "shell.execute_reply.started": "2024-04-02T16:53:56.387243Z",
          "shell.execute_reply": "2024-04-02T17:48:44.076445Z"
        },
        "trusted": true,
        "id": "WsI-F5TlCTwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_multi_metrics"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T17:50:55.547319Z",
          "iopub.execute_input": "2024-04-02T17:50:55.547806Z",
          "iopub.status.idle": "2024-04-02T17:50:55.557315Z",
          "shell.execute_reply.started": "2024-04-02T17:50:55.547757Z",
          "shell.execute_reply": "2024-04-02T17:50:55.556265Z"
        },
        "trusted": true,
        "id": "LZxh7WfvCTwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examples of Model Inferencing"
      ],
      "metadata": {
        "id": "Uoy6IDcUCTwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Model from Checkpoint"
      ],
      "metadata": {
        "id": "Dn6gontpCTws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultimodalVQAModel()\n",
        "\n",
        "# We use the checkpoint giving best results\n",
        "model.load_state_dict(torch.load(os.path.join(\"..\", \"checkpoint\", \"bert_vit\", \"checkpoint-3100\", \"pytorch_model.bin\")))\n",
        "model.to(device)"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-04-02T17:51:11.348105Z",
          "iopub.execute_input": "2024-04-02T17:51:11.348506Z",
          "iopub.status.idle": "2024-04-02T17:51:19.441011Z",
          "shell.execute_reply.started": "2024-04-02T17:51:11.348474Z",
          "shell.execute_reply": "2024-04-02T17:51:19.440228Z"
        },
        "trusted": true,
        "id": "rHOEH0GuCTws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `model` is your trained multimodal model and `multimodal_model` is the name of your model\n",
        "multimodal_model = \"MM10\"\n",
        "torch.save(model.state_dict(), f\"/kaggle/working/{multimodal_model}_model.pth\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T17:52:10.170543Z",
          "iopub.execute_input": "2024-04-02T17:52:10.170937Z",
          "iopub.status.idle": "2024-04-02T17:52:11.92509Z",
          "shell.execute_reply.started": "2024-04-02T17:52:10.170904Z",
          "shell.execute_reply": "2024-04-02T17:52:11.924211Z"
        },
        "trusted": true,
        "id": "smJCI6EuCTwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /kaggle/working\n",
        "from IPython.display import FileLink\n",
        "FileLink('MM10_model.pth')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T18:06:26.288093Z",
          "iopub.execute_input": "2024-04-02T18:06:26.288896Z",
          "iopub.status.idle": "2024-04-02T18:06:26.299279Z",
          "shell.execute_reply.started": "2024-04-02T18:06:26.288859Z",
          "shell.execute_reply": "2024-04-02T18:06:26.298399Z"
        },
        "trusted": true,
        "id": "aNKCj7ZWCTwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"/kaggle/working/MM10_model\"> Download File </a>"
      ],
      "metadata": {
        "id": "xOYfAiMbCTwu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wyDvPpF2CTwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = collator(dataset[\"test\"][2000:2025])\n",
        "\n",
        "input_ids = sample[\"input_ids\"].to(device)\n",
        "token_type_ids = sample[\"token_type_ids\"].to(device)\n",
        "attention_mask = sample[\"attention_mask\"].to(device)\n",
        "pixel_values = sample[\"pixel_values\"].to(device)\n",
        "labels = sample[\"labels\"].to(device)\n",
        "print(labels)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-02T18:03:21.19695Z",
          "iopub.execute_input": "2024-04-02T18:03:21.197494Z",
          "iopub.status.idle": "2024-04-02T18:03:21.6197Z",
          "shell.execute_reply.started": "2024-04-02T18:03:21.197454Z",
          "shell.execute_reply": "2024-04-02T18:03:21.618847Z"
        },
        "trusted": true,
        "id": "5IkMJo49CTwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pass the Samples through the Model & inspect the Predictions"
      ],
      "metadata": {
        "id": "sV-Rd1vzCTwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "output = model(input_ids, pixel_values, attention_mask, token_type_ids, labels)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T15:56:31.274708Z",
          "iopub.execute_input": "2024-04-01T15:56:31.275104Z",
          "iopub.status.idle": "2024-04-01T15:56:31.325277Z",
          "shell.execute_reply.started": "2024-04-01T15:56:31.275071Z",
          "shell.execute_reply": "2024-04-01T15:56:31.32461Z"
        },
        "trusted": true,
        "id": "ViNHJbJ1CTw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = output[\"logits\"].argmax(axis=-1).cpu().numpy()\n",
        "preds"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T15:56:31.795679Z",
          "iopub.execute_input": "2024-04-01T15:56:31.796092Z",
          "iopub.status.idle": "2024-04-01T15:56:31.806186Z",
          "shell.execute_reply.started": "2024-04-01T15:56:31.796054Z",
          "shell.execute_reply": "2024-04-01T15:56:31.805329Z"
        },
        "trusted": true,
        "id": "UgNKLZgkCTw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2000, 2025):\n",
        "    print(\"*********************************************************\")\n",
        "    showExample(train=False, id=i)\n",
        "\n",
        "    # Get the model's output logits for the current example\n",
        "    output = model(\n",
        "        input_ids=input_ids[i-2000:i-2000+1],  # Select the current example\n",
        "        pixel_values=pixel_values[i-2000:i-2000+1],\n",
        "        attention_mask=attention_mask[i-2000:i-2000+1],\n",
        "        token_type_ids=token_type_ids[i-2000:i-2000+1],\n",
        "        labels=None  # No labels provided during inference\n",
        "    )\n",
        "\n",
        "    # Get the top 5 predicted classes\n",
        "    top5_preds = torch.topk(output[\"logits\"], k=5, dim=1).indices.cpu().numpy()[0]\n",
        "\n",
        "    # Convert indices to answer space\n",
        "    top5_answers = [answer_space[pred] for pred in top5_preds]\n",
        "\n",
        "    print(\"Top 5 Predicted Answers:\")\n",
        "    for j, answer in enumerate(top5_answers):\n",
        "        print(f\"{j+1}. {answer}\")\n",
        "\n",
        "    print(\"*********************************************************\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T15:56:34.11176Z",
          "iopub.execute_input": "2024-04-01T15:56:34.112146Z",
          "iopub.status.idle": "2024-04-01T15:56:38.203895Z",
          "shell.execute_reply.started": "2024-04-01T15:56:34.112112Z",
          "shell.execute_reply": "2024-04-01T15:56:38.203061Z"
        },
        "trusted": true,
        "id": "ykAZNcYxCTw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2000, 2025):\n",
        "    print(\"*********************************************************\")\n",
        "    showExample(train=False, id=i)\n",
        "\n",
        "    # Get the model's output logits for the current example\n",
        "    output = model(\n",
        "        input_ids=input_ids[i-2000:i-2000+1],  # Select the current example\n",
        "        pixel_values=pixel_values[i-2000:i-2000+1],\n",
        "        attention_mask=attention_mask[i-2000:i-2000+1],\n",
        "        token_type_ids=token_type_ids[i-2000:i-2000+1],\n",
        "        labels=None  # No labels provided during inference\n",
        "    )\n",
        "\n",
        "    # Get the top 5 predicted classes and their corresponding logits\n",
        "    top5_preds = torch.topk(output[\"logits\"], k=5, dim=1)\n",
        "    top5_indices = top5_preds.indices.cpu().numpy()[0]\n",
        "    #top5_logits = top5_preds.values.cpu().numpy()[0]\n",
        "    top5_logits = top5_preds.values.detach().cpu().numpy()[0]\n",
        "\n",
        "\n",
        "    # Convert indices to answer space and print along with confidence\n",
        "    print(\"Top 5 Predicted Answers with Confidence:\")\n",
        "    for j, (pred_idx, logit) in enumerate(zip(top5_indices, top5_logits)):\n",
        "        answer = answer_space[pred_idx]\n",
        "        print(f\"{j+1}. {answer} (Confidence: {logit:.2f})\")\n",
        "\n",
        "    print(\"*********************************************************\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-22T11:37:42.382025Z",
          "iopub.execute_input": "2024-03-22T11:37:42.382315Z",
          "iopub.status.idle": "2024-03-22T11:37:46.402597Z",
          "shell.execute_reply.started": "2024-03-22T11:37:42.382287Z",
          "shell.execute_reply": "2024-03-22T11:37:46.401859Z"
        },
        "trusted": true,
        "id": "0_V_QnzYCTw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After using softmax to bound confidence"
      ],
      "metadata": {
        "id": "pAYMnK7BCTw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2000, 2025):\n",
        "    print(\"*********************************************************\")\n",
        "    showExample(train=False, id=i)\n",
        "\n",
        "    # Get the model's output logits for the current example\n",
        "    output = model(\n",
        "        input_ids=input_ids[i-2000:i-2000+1],  # Select the current example\n",
        "        pixel_values=pixel_values[i-2000:i-2000+1],\n",
        "        attention_mask=attention_mask[i-2000:i-2000+1],\n",
        "        token_type_ids=token_type_ids[i-2000:i-2000+1],\n",
        "        labels=None  # No labels provided during inference\n",
        "    )\n",
        "\n",
        "    # Get the top 5 predicted classes and their corresponding logits\n",
        "    top5_preds = torch.topk(output[\"logits\"], k=5, dim=1)\n",
        "    top5_indices = top5_preds.indices.cpu().numpy()[0]\n",
        "    #top5_logits = top5_preds.values.cpu().numpy()[0]\n",
        "    top5_logits = top5_preds.values.detach().cpu().numpy()[0]\n",
        "    # Apply softmax to logits to get probabilities\n",
        "    probabilities = torch.nn.functional.softmax(torch.tensor(top5_logits), dim=0)\n",
        "\n",
        "    # Convert indices to answer space and print along with confidence\n",
        "    print(\"Top 5 Predicted Answers with Confidence:\")\n",
        "    for j, (pred_idx, probability) in enumerate(zip(top5_indices, probabilities)):\n",
        "        answer = answer_space[pred_idx]\n",
        "        confidence = probability.item() * 100\n",
        "        print(f\"{j+1}. {answer} (Confidence: {confidence:.2f}%)\")\n",
        "\n",
        "\n",
        "    # Convert indices to answer space and print along with confidence\n",
        "#     print(\"Top 5 Predicted Answers with Confidence:\")\n",
        "#     for j, (pred_idx, logit) in enumerate(zip(top5_indices, top5_logits)):\n",
        "#         answer = answer_space[pred_idx]\n",
        "#         print(f\"{j+1}. {answer} (Confidence: {logit:.2f})\")\n",
        "\n",
        "#     print(\"*********************************************************\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T15:57:29.098252Z",
          "iopub.execute_input": "2024-04-01T15:57:29.098662Z",
          "iopub.status.idle": "2024-04-01T15:57:33.230847Z",
          "shell.execute_reply.started": "2024-04-01T15:57:29.09863Z",
          "shell.execute_reply": "2024-04-01T15:57:33.229927Z"
        },
        "trusted": true,
        "id": "gfSGTscbCTw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to create colored progress bar\n",
        "def progress_bar(percentage):\n",
        "    bar_length = 20\n",
        "    filled_length = int(bar_length * percentage / 100)\n",
        "    bar = '[' + '=' * filled_length + ' ' * (bar_length - filled_length) + ']'\n",
        "    return bar\n",
        "\n",
        "for i in range(2000, 2025):\n",
        "    print(\"*********************************************************\")\n",
        "    showExample(train=False, id=i)\n",
        "\n",
        "    # Get the model's output logits for the current example\n",
        "    output = model(\n",
        "        input_ids=input_ids[i-2000:i-2000+1],  # Select the current example\n",
        "        pixel_values=pixel_values[i-2000:i-2000+1],\n",
        "        attention_mask=attention_mask[i-2000:i-2000+1],\n",
        "        token_type_ids=token_type_ids[i-2000:i-2000+1],\n",
        "        labels=None  # No labels provided during inference\n",
        "    )\n",
        "\n",
        "    # Get the top 5 predicted classes and their corresponding logits\n",
        "    top5_preds = torch.topk(output[\"logits\"], k=5, dim=1)\n",
        "    top5_indices = top5_preds.indices.cpu().numpy()[0]\n",
        "    #top5_logits = top5_preds.values.cpu().numpy()[0]\n",
        "    top5_logits = top5_preds.values.detach().cpu().numpy()[0]\n",
        "    # Apply softmax to logits to get probabilities\n",
        "    probabilities = torch.nn.functional.softmax(torch.tensor(top5_logits), dim=0)\n",
        "\n",
        "\n",
        "    # Convert indices to answer space and print along with confidence\n",
        "    print(\"Top 5 Predicted Answers with Confidence:\")\n",
        "    for j, (pred_idx, probability) in enumerate(zip(top5_indices, probabilities)):\n",
        "        answer = answer_space[pred_idx]\n",
        "        confidence = probability.item() * 100\n",
        "        print(f\"{j+1}. {answer}:\")\n",
        "        print(f\"   Confidence: {confidence:.2f}%\")\n",
        "        print(\"   Progress: \" + progress_bar(confidence))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T16:01:36.054557Z",
          "iopub.execute_input": "2024-04-01T16:01:36.054981Z",
          "iopub.status.idle": "2024-04-01T16:01:40.254596Z",
          "shell.execute_reply.started": "2024-04-01T16:01:36.054948Z",
          "shell.execute_reply": "2024-04-01T16:01:40.253741Z"
        },
        "trusted": true,
        "id": "GjRB9QdRCTw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2000, 2025):\n",
        "    print(\"*********************************************************\")\n",
        "    showExample(train=False, id=i)\n",
        "    print(\"Predicted Answer:\\t\", answer_space[preds[i-2000]])\n",
        "    print(\"*********************************************************\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T16:01:40.25613Z",
          "iopub.execute_input": "2024-04-01T16:01:40.256945Z"
        },
        "trusted": true,
        "id": "HA461fUACTw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to create colored progress bar\n",
        "def progress_bar(percentage):\n",
        "    bar_length = 20\n",
        "    filled_length = int(bar_length * percentage / 100)\n",
        "    bar = '[' + '=' * filled_length + ' ' * (bar_length - filled_length) + ']'\n",
        "    return bar\n",
        "\n",
        "def colorize_progress_bar(percentage):\n",
        "    color_map = [\n",
        "        (80, '#'),  # Green for confidence >= 80%\n",
        "        (60, '+'),  # Yellow for confidence >= 60%\n",
        "        (0, '-')   # Red for confidence < 60%\n",
        "    ]\n",
        "    color_char = ''\n",
        "    for color_thresh, char in color_map:\n",
        "        if percentage >= color_thresh:\n",
        "            color_char = char\n",
        "            break\n",
        "    return f\"{color_char}{progress_bar(percentage)}\"\n",
        "\n",
        "for i in range(2000, 2025):\n",
        "    print(\"*********************************************************\")\n",
        "    showExample(train=False, id=i)\n",
        "\n",
        "    # Get the model's output logits for the current example\n",
        "    output = model(\n",
        "        input_ids=input_ids[i-2000:i-2000+1],  # Select the current example\n",
        "        pixel_values=pixel_values[i-2000:i-2000+1],\n",
        "        attention_mask=attention_mask[i-2000:i-2000+1],\n",
        "        token_type_ids=token_type_ids[i-2000:i-2000+1],\n",
        "        labels=None  # No labels provided during inference\n",
        "    )\n",
        "\n",
        "    # Get the top 5 predicted classes and their corresponding logits\n",
        "    top5_preds = torch.topk(output[\"logits\"], k=5, dim=1)\n",
        "    top5_indices = top5_preds.indices.cpu().numpy()[0]\n",
        "    #top5_logits = top5_preds.values.cpu().numpy()[0]\n",
        "    top5_logits = top5_preds.values.detach().cpu().numpy()[0]\n",
        "    # Apply softmax to logits to get probabilities\n",
        "    probabilities = torch.nn.functional.softmax(torch.tensor(top5_logits), dim=0)\n",
        "\n",
        "    # Convert indices to answer space and print along with confidence\n",
        "    print(\"Top 5 Predicted Answers with Confidence:\")\n",
        "    for j, (pred_idx, probability) in enumerate(zip(top5_indices, probabilities)):\n",
        "        answer = answer_space[pred_idx]\n",
        "        confidence = probability.item() * 100\n",
        "        print(f\"{j+1}. {answer}:\")\n",
        "        print(f\"   Confidence: {confidence:.2f}%\")\n",
        "        print(\"   Progress: \" + colorize_progress_bar(confidence))\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-22T11:44:55.15374Z",
          "iopub.execute_input": "2024-03-22T11:44:55.154165Z",
          "iopub.status.idle": "2024-03-22T11:44:59.174557Z",
          "shell.execute_reply.started": "2024-03-22T11:44:55.15413Z",
          "shell.execute_reply": "2024-03-22T11:44:59.173758Z"
        },
        "trusted": true,
        "id": "gpFM-z9xCTw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to create colored progress bar\n",
        "def progress_bar(percentage):\n",
        "    bar_length = 20\n",
        "    filled_length = int(bar_length * percentage / 100)\n",
        "    bar = '[' + '=' * filled_length + ' ' * (bar_length - filled_length) + ']'\n",
        "    return bar\n",
        "\n",
        "def colorize_progress_bar(percentage):\n",
        "    color = 'green' if percentage >= 75 else ('orange' if percentage >= 50 else 'red')\n",
        "    return f'\\033[1;30;{color}m{progress_bar(percentage)}\\033[m'\n",
        "\n",
        "for i in range(2000, 2025):\n",
        "    print(\"*********************************************************\")\n",
        "    showExample(train=False, id=i)\n",
        "\n",
        "    # Get the model's output logits for the current example\n",
        "    output = model(\n",
        "        input_ids=input_ids[i-2000:i-2000+1],  # Select the current example\n",
        "        pixel_values=pixel_values[i-2000:i-2000+1],\n",
        "        attention_mask=attention_mask[i-2000:i-2000+1],\n",
        "        token_type_ids=token_type_ids[i-2000:i-2000+1],\n",
        "        labels=None  # No labels provided during inference\n",
        "    )\n",
        "\n",
        "    # Get the top 5 predicted classes and their corresponding logits\n",
        "    top5_preds = torch.topk(output[\"logits\"], k=5, dim=1)\n",
        "    top5_indices = top5_preds.indices.cpu().numpy()[0]\n",
        "    #top5_logits = top5_preds.values.cpu().numpy()[0]\n",
        "    top5_logits = top5_preds.values.detach().cpu().numpy()[0]\n",
        "    # Apply softmax to logits to get probabilities\n",
        "    probabilities = torch.nn.functional.softmax(torch.tensor(top5_logits), dim=0)\n",
        "\n",
        "    # Convert indices to answer space and print along with confidence\n",
        "    print(\"Top 5 Predicted Answers with Confidence:\")\n",
        "    for j, (pred_idx, probability) in enumerate(zip(top5_indices, probabilities)):\n",
        "        answer = answer_space[pred_idx]\n",
        "        confidence = probability.item() * 100\n",
        "        print(f\"{j+1}. {answer}:\")\n",
        "        print(f\"   Confidence: {confidence:.2f}%\")\n",
        "        print(\"   Progress: \" + colorize_progress_bar(confidence))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-22T11:42:22.601316Z",
          "iopub.execute_input": "2024-03-22T11:42:22.601783Z",
          "iopub.status.idle": "2024-03-22T11:42:26.663368Z",
          "shell.execute_reply.started": "2024-03-22T11:42:22.601747Z",
          "shell.execute_reply": "2024-03-22T11:42:26.662494Z"
        },
        "trusted": true,
        "id": "pjLmTugBCTw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def similarity(a, b):\n",
        "    # Split words if it is a list and remove extra spaces\n",
        "    words_a = [w.strip() for w in a.split(',')]\n",
        "    words_b = [w.strip() for w in b.split(',')]\n",
        "\n",
        "    # Split words if connected by underscore _\n",
        "    a = [w_ for word in words_a for w_ in word.split('_')]\n",
        "    b = [w_ for word in words_b for w_ in word.split('_')]\n",
        "\n",
        "    res = 0\n",
        "    n = 0\n",
        "\n",
        "    # Calculate score and take average\n",
        "    for i in a:\n",
        "        synsets_i = wordnet.synsets(i)\n",
        "        if synsets_i:\n",
        "            s1 = synsets_i[0]\n",
        "            for j in b:\n",
        "                synsets_j = wordnet.synsets(j)\n",
        "                if synsets_j:\n",
        "                    s2 = synsets_j[0]\n",
        "                    sim = s1.wup_similarity(s2)\n",
        "                    if sim:\n",
        "                        res += sim\n",
        "                    n += 1\n",
        "\n",
        "    return res / n if n != 0 else 0\n",
        "\n",
        "# Show predictions for a range of examples\n",
        "for i in range(2000, 2005):\n",
        "    print(\"\\n=========================================================\\n\")\n",
        "    real_answer = showExample(train=False, id=i)\n",
        "    predicted_answer = answer_space[preds[i - 2000]]\n",
        "    print(\"Predicted Answer:\\t\", predicted_answer)\n",
        "    print(f\"Similarity: {similarity(real_answer, predicted_answer)}\")\n",
        "    print(\"\\n=========================================================\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T12:28:38.685134Z",
          "iopub.execute_input": "2024-03-20T12:28:38.685436Z",
          "iopub.status.idle": "2024-03-20T12:28:38.948538Z",
          "shell.execute_reply.started": "2024-03-20T12:28:38.685407Z",
          "shell.execute_reply": "2024-03-20T12:28:38.947554Z"
        },
        "trusted": true,
        "id": "waTrCilJCTw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting Model Size"
      ],
      "metadata": {
        "id": "VZ2XvQ3PCTw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def countTrainableParameters(model):\n",
        "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(\"No. of trainable parameters:\\t{0:,}\".format(num_params))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T12:28:40.707684Z",
          "iopub.execute_input": "2024-03-20T12:28:40.708531Z",
          "iopub.status.idle": "2024-03-20T12:28:40.714199Z",
          "shell.execute_reply.started": "2024-03-20T12:28:40.708495Z",
          "shell.execute_reply": "2024-03-20T12:28:40.713398Z"
        },
        "trusted": true,
        "id": "FhlZwakKCTw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "countTrainableParameters(model) # For BERT-ViT model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T12:28:42.471934Z",
          "iopub.execute_input": "2024-03-20T12:28:42.472796Z",
          "iopub.status.idle": "2024-03-20T12:28:42.481175Z",
          "shell.execute_reply.started": "2024-03-20T12:28:42.472756Z",
          "shell.execute_reply": "2024-03-20T12:28:42.480291Z"
        },
        "trusted": true,
        "id": "ixKuVRh4CTw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `model` is your trained multimodal model and `multimodal_model` is the name of your model\n",
        "multimodal_model = \"MM30\"\n",
        "torch.save(model.state_dict(), f\"/kaggle/working/{multimodal_model}_model.pth\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T16:06:34.400938Z",
          "iopub.execute_input": "2024-04-01T16:06:34.401705Z",
          "iopub.status.idle": "2024-04-01T16:06:36.331908Z",
          "shell.execute_reply.started": "2024-04-01T16:06:34.401665Z",
          "shell.execute_reply": "2024-04-01T16:06:36.330808Z"
        },
        "trusted": true,
        "id": "9k5qeh80CTw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {'model': MultimodalVQAModel(),\n",
        "              'state_dict': model.state_dict()}\n",
        "\n",
        "torch.save(checkpoint, '/kaggle/working/checkpoint.pth')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T16:06:51.914911Z",
          "iopub.execute_input": "2024-04-01T16:06:51.915324Z",
          "iopub.status.idle": "2024-04-01T16:07:03.861697Z",
          "shell.execute_reply.started": "2024-04-01T16:06:51.91529Z",
          "shell.execute_reply": "2024-04-01T16:07:03.860621Z"
        },
        "trusted": true,
        "id": "h-FEIIJkCTw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = checkpoint['model']\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    for parameter in model.parameters():\n",
        "        parameter.requires_grad = False\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T12:29:16.867091Z",
          "iopub.execute_input": "2024-03-20T12:29:16.867505Z",
          "iopub.status.idle": "2024-03-20T12:29:16.874281Z",
          "shell.execute_reply.started": "2024-03-20T12:29:16.867464Z",
          "shell.execute_reply": "2024-03-20T12:29:16.87343Z"
        },
        "trusted": true,
        "id": "3wyfhrdeCTw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = load_checkpoint('/kaggle/working/checkpoint.pth')\n",
        "model1.to(device)\n",
        "print(model1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T12:29:19.166863Z",
          "iopub.execute_input": "2024-03-20T12:29:19.167247Z",
          "iopub.status.idle": "2024-03-20T12:29:20.650215Z",
          "shell.execute_reply.started": "2024-03-20T12:29:19.167212Z",
          "shell.execute_reply": "2024-03-20T12:29:20.649371Z"
        },
        "trusted": true,
        "id": "zKkGFlsvCTw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = collator(dataset[\"test\"][2000:2025])\n",
        "\n",
        "input_ids = input_ids.to(device)\n",
        "pixel_values = pixel_values.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "token_type_ids = token_type_ids.to(device)\n",
        "labels = labels.to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T12:32:08.994943Z",
          "iopub.execute_input": "2024-03-20T12:32:08.995329Z",
          "iopub.status.idle": "2024-03-20T12:32:09.354431Z",
          "shell.execute_reply.started": "2024-03-20T12:32:08.995296Z",
          "shell.execute_reply": "2024-03-20T12:32:09.353569Z"
        },
        "trusted": true,
        "id": "DWKanX_ICTw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.eval()\n",
        "output = model(input_ids, pixel_values, attention_mask, token_type_ids, labels)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T12:32:09.562861Z",
          "iopub.execute_input": "2024-03-20T12:32:09.563203Z",
          "iopub.status.idle": "2024-03-20T12:32:09.597476Z",
          "shell.execute_reply.started": "2024-03-20T12:32:09.563173Z",
          "shell.execute_reply": "2024-03-20T12:32:09.59684Z"
        },
        "trusted": true,
        "id": "cpgjwwBuCTw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = output[\"logits\"].argmax(axis=-1).cpu().numpy()\n",
        "preds"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T12:32:10.253992Z",
          "iopub.execute_input": "2024-03-20T12:32:10.254762Z",
          "iopub.status.idle": "2024-03-20T12:32:10.263662Z",
          "shell.execute_reply.started": "2024-03-20T12:32:10.254725Z",
          "shell.execute_reply": "2024-03-20T12:32:10.262792Z"
        },
        "trusted": true,
        "id": "2SfxGtZdCTw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2000, 2025):\n",
        "    print(\"*********************************************************\")\n",
        "    showExample(train=False, id=i)\n",
        "    print(\"Predicted Answer:\\t\", answer_space[preds[i-2000]])\n",
        "    print(\"*********************************************************\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T12:32:11.081829Z",
          "iopub.execute_input": "2024-03-20T12:32:11.082726Z",
          "iopub.status.idle": "2024-03-20T12:32:14.586068Z",
          "shell.execute_reply.started": "2024-03-20T12:32:11.082685Z",
          "shell.execute_reply": "2024-03-20T12:32:14.585141Z"
        },
        "trusted": true,
        "id": "Oc7zsasJCTw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "url = \"https://images.unsplash.com/photo-1707817812089-586ca2bfe711?w=500&auto=format&fit=crop&q=60&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxlZGl0b3JpYWwtZmVlZHwxOXx8fGVufDB8fHx8fA%3D%3D\"\n",
        "response = requests.get(url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "img"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T12:34:02.424799Z",
          "iopub.execute_input": "2024-03-20T12:34:02.425758Z",
          "iopub.status.idle": "2024-03-20T12:34:02.728315Z",
          "shell.execute_reply.started": "2024-03-20T12:34:02.42571Z",
          "shell.execute_reply": "2024-03-20T12:34:02.727496Z"
        },
        "trusted": true,
        "id": "2hjqAiVgCTxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your model requires a specific input size\n",
        "img = img.resize((224, 224))\n",
        "img"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.687553Z",
          "iopub.status.idle": "2024-03-20T02:17:42.687901Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.687744Z",
          "shell.execute_reply": "2024-03-20T02:17:42.68776Z"
        },
        "trusted": true,
        "id": "-9I5CLCmCTxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Add other transformations if needed\n",
        "])\n",
        "\n",
        "input_image = transform(img).unsqueeze(0)  # Add batch dimension\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.688962Z",
          "iopub.status.idle": "2024-03-20T02:17:42.689275Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.689122Z",
          "shell.execute_reply": "2024-03-20T02:17:42.689137Z"
        },
        "trusted": true,
        "id": "GJ49j3WECTxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoFeatureExtractor\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "url = \"https://images.unsplash.com/photo-1707817812089-586ca2bfe711?w=500&auto=format&fit=crop&q=60&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxlZGl0b3JpYWwtZmVlZHwxOXx8fGVufDB8fHx8fA%3D%3D\"\n",
        "response = requests.get(url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "img = img.resize((224, 224))\n",
        "# # Replace \"your_image_url\" with the URL of the image you want to use\n",
        "# image_url = \"your_image_url\"\n",
        "\n",
        "# # Download the image and preprocess it\n",
        "# # ... (you need to implement this part based on your requirements)\n",
        "\n",
        "# # Tokenize the text (you can use any text since you're working with an image)\n",
        "# text = \"How many people are there\"\n",
        "# inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Assuming \"pixel_values\" is your preprocessed image tensor\n",
        "# \"attention_mask\" and \"token_type_ids\" are obtained from tokenizer output\n",
        "with torch.no_grad():\n",
        "    output = model1(input_image, pixel_values, attention_mask, token_type_ids, labels)\n",
        "\n",
        "# Process the output as needed"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.690315Z",
          "iopub.status.idle": "2024-03-20T02:17:42.690688Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.690489Z",
          "shell.execute_reply": "2024-03-20T02:17:42.690505Z"
        },
        "trusted": true,
        "id": "_syiW7dYCTxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.691722Z",
          "iopub.status.idle": "2024-03-20T02:17:42.692035Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.691882Z",
          "shell.execute_reply": "2024-03-20T02:17:42.691898Z"
        },
        "trusted": true,
        "id": "V-HcW1ARCTxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Assuming `collator`, `model`, and `answer_space` are defined as in your previous code\n",
        "\n",
        "def predict(input_text, input_image):\n",
        "    # Tokenize input_text and process input_image\n",
        "    sample = collator({\"question\": input_text, \"image\": input_image})\n",
        "\n",
        "    input_ids = sample[\"input_ids\"].to(device)\n",
        "    token_type_ids = sample[\"token_type_ids\"].to(device)\n",
        "    attention_mask = sample[\"attention_mask\"].to(device)\n",
        "    pixel_values = sample[\"pixel_values\"].to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids, pixel_values, attention_mask, token_type_ids)\n",
        "\n",
        "    # Get predicted answer index\n",
        "    pred_index = output[\"logits\"].argmax(axis=-1).item()\n",
        "\n",
        "    return answer_space[pred_index]\n",
        "\n",
        "iface = gr.Interface(fn=predict,\n",
        "                     inputs=[\"text\", \"image\"],\n",
        "                     outputs=\"text\",\n",
        "                     live=True,\n",
        "                     capture_session=True)\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.693424Z",
          "iopub.status.idle": "2024-03-20T02:17:42.69389Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.693658Z",
          "shell.execute_reply": "2024-03-20T02:17:42.693681Z"
        },
        "trusted": true,
        "id": "vx08Fx3tCTxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade huggingface_hub"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.69556Z",
          "iopub.status.idle": "2024-03-20T02:17:42.69605Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.695809Z",
          "shell.execute_reply": "2024-03-20T02:17:42.695832Z"
        },
        "trusted": true,
        "id": "uFncYhrDCTxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio==3.14.0 huggingface_hub==0.0.15"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.697201Z",
          "iopub.status.idle": "2024-03-20T02:17:42.697693Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.697427Z",
          "shell.execute_reply": "2024-03-20T02:17:42.69745Z"
        },
        "trusted": true,
        "id": "XEJd6aKNCTxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "\n",
        "# Load the model\n",
        "model = MultimodalVQAModel()\n",
        "model.load_state_dict(torch.load(\"MM1_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Assuming `collator` is your data collator function as defined earlier\n",
        "\n",
        "def predict(input_text, input_image):\n",
        "    # Tokenize input_text and process input_image\n",
        "    sample = collator({\"question\": input_text, \"image\": input_image})\n",
        "\n",
        "    input_ids = sample[\"input_ids\"].to(device)\n",
        "    token_type_ids = sample[\"token_type_ids\"].to(device)\n",
        "    attention_mask = sample[\"attention_mask\"].to(device)\n",
        "    pixel_values = sample[\"pixel_values\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids, pixel_values, attention_mask, token_type_ids)\n",
        "\n",
        "    # Get predicted answer index\n",
        "    pred_index = output[\"logits\"].argmax(axis=-1).item()\n",
        "\n",
        "    return answer_space[pred_index]\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(fn=predict,\n",
        "                     inputs=[\"text\", \"image\"],\n",
        "                     outputs=\"text\",\n",
        "                     live=True,\n",
        "                     capture_session=True\n",
        "                     )\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.699169Z",
          "iopub.status.idle": "2024-03-20T02:17:42.699645Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.699393Z",
          "shell.execute_reply": "2024-03-20T02:17:42.699415Z"
        },
        "trusted": true,
        "id": "8gQIg5MWCTxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.701172Z",
          "iopub.status.idle": "2024-03-20T02:17:42.701734Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.701468Z",
          "shell.execute_reply": "2024-03-20T02:17:42.701493Z"
        },
        "trusted": true,
        "id": "D9mhc9reCTxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import torch\n",
        "\n",
        "# Assuming `collator` is your data collator function as defined earlier\n",
        "# Assuming `MultimodalVQAModel` is your model class\n",
        "\n",
        "# Load the model\n",
        "model = MultimodalVQAModel()\n",
        "model.load_state_dict(torch.load(\"MM2_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Define the prediction function\n",
        "def predict(input_text, input_image):\n",
        "    # Tokenize input_text and process input_image\n",
        "    sample = collator({\"question\": input_text, \"image\": input_image})\n",
        "\n",
        "    input_ids = sample[\"input_ids\"].to(device)\n",
        "    token_type_ids = sample[\"token_type_ids\"].to(device)\n",
        "    attention_mask = sample[\"attention_mask\"].to(device)\n",
        "    pixel_values = sample[\"pixel_values\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids, pixel_values, attention_mask, token_type_ids)\n",
        "\n",
        "    # Get predicted answer index\n",
        "    pred_index = output[\"logits\"].argmax(axis=-1).item()\n",
        "\n",
        "    return answer_space[pred_index]\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    st.title(\"Multimodal QA Prediction\")\n",
        "\n",
        "    # Input components\n",
        "    input_text = st.text_area(\"Enter Question:\")\n",
        "    input_image = st.file_uploader(\"Upload Image:\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    # Make prediction on button click\n",
        "    if st.button(\"Predict\"):\n",
        "        if input_text and input_image:\n",
        "            result = predict(input_text, input_image)\n",
        "            st.success(f\"Predicted Answer: {result}\")\n",
        "        else:\n",
        "            st.warning(\"Please provide both text and image inputs.\")\n",
        "\n",
        "# Run the Streamlit app\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.702714Z",
          "iopub.status.idle": "2024-03-20T02:17:42.703055Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.702889Z",
          "shell.execute_reply": "2024-03-20T02:17:42.702906Z"
        },
        "trusted": true,
        "id": "gyCir8hlCTxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "st.write(\"\"\"import streamlit as st\n",
        "import torch\n",
        "\n",
        "# Assuming `collator` is your data collator function as defined earlier\n",
        "# Assuming `MultimodalVQAModel` is your model class\n",
        "\n",
        "# Load the model\n",
        "model = MultimodalVQAModel()\n",
        "model.load_state_dict(torch.load(\"MM2_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Define the prediction function\n",
        "def predict(input_text, input_image):\n",
        "    # Tokenize input_text and process input_image\n",
        "    sample = collator({\"question\": input_text, \"image\": input_image})\n",
        "\n",
        "    input_ids = sample[\"input_ids\"].to(device)\n",
        "    token_type_ids = sample[\"token_type_ids\"].to(device)\n",
        "    attention_mask = sample[\"attention_mask\"].to(device)\n",
        "    pixel_values = sample[\"pixel_values\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids, pixel_values, attention_mask, token_type_ids)\n",
        "\n",
        "    # Get predicted answer index\n",
        "    pred_index = output[\"logits\"].argmax(axis=-1).item()\n",
        "\n",
        "    return answer_space[pred_index]\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    st.title(\"Multimodal QA Prediction\")\n",
        "\n",
        "    # Input components\n",
        "    input_text = st.text_area(\"Enter Question:\")\n",
        "    input_image = st.file_uploader(\"Upload Image:\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    # Make prediction on button click\n",
        "    if st.button(\"Predict\"):\n",
        "        if input_text and input_image:\n",
        "            result = predict(input_text, input_image)\n",
        "            st.success(f\"Predicted Answer: {result}\")\n",
        "        else:\n",
        "            st.warning(\"Please provide both text and image inputs.\")\n",
        "\n",
        "# Run the Streamlit app\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.704191Z",
          "iopub.status.idle": "2024-03-20T02:17:42.704505Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.704351Z",
          "shell.execute_reply": "2024-03-20T02:17:42.704366Z"
        },
        "trusted": true,
        "id": "R3VvWiDiCTxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LtcQ4J5zCTxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - https://loca.lt/mytunnelpassword"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.705665Z",
          "iopub.status.idle": "2024-03-20T02:17:42.706012Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.705847Z",
          "shell.execute_reply": "2024-03-20T02:17:42.705864Z"
        },
        "trusted": true,
        "id": "jPZe0AteCTxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8502"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.707059Z",
          "iopub.status.idle": "2024-03-20T02:17:42.707386Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.707229Z",
          "shell.execute_reply": "2024-03-20T02:17:42.707244Z"
        },
        "trusted": true,
        "id": "qfmemYbOCTxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!npm install localtunnel"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.708319Z",
          "iopub.status.idle": "2024-03-20T02:17:42.708679Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.708489Z",
          "shell.execute_reply": "2024-03-20T02:17:42.708506Z"
        },
        "trusted": true,
        "id": "yh68fskuCTxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your public ip is the password to the localtunnel\n",
        "!curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.709444Z",
          "iopub.status.idle": "2024-03-20T02:17:42.709797Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.709631Z",
          "shell.execute_reply": "2024-03-20T02:17:42.709649Z"
        },
        "trusted": true,
        "id": "o03F1IcQCTxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>./logs.txt & npx localtunnel --port 8501"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T02:17:42.711235Z",
          "iopub.status.idle": "2024-03-20T02:17:42.711549Z",
          "shell.execute_reply.started": "2024-03-20T02:17:42.711398Z",
          "shell.execute_reply": "2024-03-20T02:17:42.711413Z"
        },
        "trusted": true,
        "id": "vD2mBcPGCTxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalVQAModel(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_labels: int = len(answer_space),\n",
        "            intermediate_dim: int = 512,\n",
        "            pretrained_text_name: str = 'bert-base-uncased',\n",
        "            pretrained_image_name: str = 'google/vit-base-patch16-224-in21k'):\n",
        "\n",
        "        super(MultimodalVQAModel, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.pretrained_text_name = pretrained_text_name\n",
        "        self.pretrained_image_name = pretrained_image_name\n",
        "\n",
        "        self.text_encoder = AutoModel.from_pretrained(\n",
        "            self.pretrained_text_name,\n",
        "        )\n",
        "        self.image_encoder = AutoModel.from_pretrained(\n",
        "            self.pretrained_image_name,\n",
        "        )\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size, intermediate_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids: torch.LongTensor,\n",
        "            pixel_values: torch.FloatTensor,\n",
        "            attention_mask: Optional[torch.LongTensor] = None,\n",
        "            token_type_ids: Optional[torch.LongTensor] = None,\n",
        "            labels: Optional[torch.LongTensor] = None):\n",
        "\n",
        "        encoded_text = self.text_encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        encoded_image = self.image_encoder(\n",
        "            pixel_values=pixel_values,\n",
        "            return_dict=True,\n",
        "        )\n",
        "        fused_output = self.fusion(\n",
        "            torch.cat([encoded_text['pooler_output'], encoded_image['pooler_output'],],dim=1)\n",
        "        )\n",
        "        logits = self.classifier(fused_output)\n",
        "\n",
        "        out = {\n",
        "            \"logits\": logits\n",
        "        }\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(logits, labels)\n",
        "            out[\"loss\"] = loss\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-20T09:59:21.902115Z",
          "iopub.execute_input": "2024-03-20T09:59:21.902931Z",
          "iopub.status.idle": "2024-03-20T09:59:21.91549Z",
          "shell.execute_reply.started": "2024-03-20T09:59:21.902894Z",
          "shell.execute_reply": "2024-03-20T09:59:21.914707Z"
        },
        "trusted": true,
        "id": "08Rag8nwCTxH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}